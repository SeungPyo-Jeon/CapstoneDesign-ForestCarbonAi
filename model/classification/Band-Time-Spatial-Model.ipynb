{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.TransposeTransformer import TransposeTransformer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from utils.utils import train_model, evaluate_model_with_cm, TiffDataset, base_transform,he_init_weights\n",
    "from models.video_classifier import VideoClassifier, BandExpansion, ResBlock, FeedForward,init\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "source_data_dir = rf\"C:\\Users\\전승표\\서울과기대\\JupyterNotebook\\Capstone\\git_folder\\data_final\\source_data\"\n",
    "label_file_path = rf\"C:\\Users\\전승표\\서울과기대\\JupyterNotebook\\Capstone\\git_folder\\data_final\\label_data\\species\\label_mapping_sampled.csv\"\n",
    "test_filter = lambda x: x >= 1 and (x % 50) in {1, 6, 13, 18, 25, 30, 32, 37, 44, 49}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "bands = 10\n",
    "patch_size = 3 # 모델 안에서 3*3으로 변경\n",
    "\n",
    "train_dataset = TiffDataset(\n",
    "    large_tif_dir = os.path.join(source_data_dir,\"with_s2_uint16_revised\"),\n",
    "    file_list = [\"jiri_1.tif\", \"jiri_2.tif\", \"sobaek.tif\"], #전체 지역을 모두 사용한다.\n",
    "    label_file = label_file_path,\n",
    "    box_filter_fn = lambda box_number: not test_filter(box_number),\n",
    "    transform=base_transform(bands=bands, patch_size=patch_size),\n",
    "    patch_size = patch_size\n",
    ")\n",
    "val_dataset = TiffDataset(\n",
    "    large_tif_dir = os.path.join(source_data_dir,\"with_s2_uint16_revised\"),\n",
    "    file_list = [\"jiri_1.tif\", \"jiri_2.tif\", \"sobaek.tif\"], #전체 지역을 모두 사용한다.\n",
    "    label_file = label_file_path,\n",
    "    box_filter_fn = lambda box_number: test_filter(box_number),\n",
    "    transform=base_transform(bands=bands, patch_size=patch_size),\n",
    "    patch_size = patch_size\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파라미터 개수: 456374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training:   2%|▏         | 128/5709 [00:05<03:39, 25.46it/s]"
     ]
    }
   ],
   "source": [
    "model = TransposeTransformer(num_blocks=4, \n",
    "                 num_head=10,\n",
    "                 num_axis=12, \n",
    "                 num_dim=9,\n",
    "                 num_classes=6, \n",
    "                 image_size=3,\n",
    "                 patch_size=1, \n",
    "                 expand_ratio=2, \n",
    "                 droppath=0.2, \n",
    "                 dropout=0.1,device=device).to(device)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.05)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader)*epochs)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"파라미터 개수: {total_params}\")\n",
    "best_model_state, train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer,scheduler=scheduler, num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time torch.Size([1, 10, 12, 9])\n",
      "transformer time\n",
      "band torch.Size([1, 12, 10, 9])\n",
      "transformer band\n",
      "spatial torch.Size([1, 12, 9, 10])\n",
      "transformer spatial\n",
      "time torch.Size([1, 10, 12, 9])\n",
      "transformer time\n",
      "band torch.Size([1, 12, 10, 9])\n",
      "transformer band\n",
      "spatial torch.Size([1, 12, 9, 10])\n",
      "transformer spatial\n",
      "torch.Size([1, 6]) tensor([[ 0.2583, -1.2302, -1.3778, -0.0181, -0.2155,  0.2224]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.layers import DropPath, trunc_normal_\n",
    "import math\n",
    "from models.TransposeTransformer import TransposeTransformer\n",
    "\n",
    "import numpy as np\n",
    "model = TransposeTransformer(num_blocks=2, \n",
    "                 num_head=10,\n",
    "                 num_axis=12, \n",
    "                 num_dim=9,\n",
    "                 num_classes=6, \n",
    "                 image_size=3,\n",
    "                 patch_size=1, \n",
    "                 expand_ratio=2, \n",
    "                 droppath=0.0, \n",
    "                 dropout=0.0)\n",
    "\n",
    "a = torch.tensor( np.arange(0,12*10*9) , dtype=torch.float32).view(1,10, 12, 3,3)\n",
    "out = model(a)\n",
    "print(out.shape, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 3, 3]) tensor([[[[[ 0,  1,  2],\n",
      "           [ 3,  4,  5],\n",
      "           [ 6,  7,  8]],\n",
      "\n",
      "          [[ 9, 10, 11],\n",
      "           [12, 13, 14],\n",
      "           [15, 16, 17]],\n",
      "\n",
      "          [[18, 19, 20],\n",
      "           [21, 22, 23],\n",
      "           [24, 25, 26]]],\n",
      "\n",
      "\n",
      "         [[[27, 28, 29],\n",
      "           [30, 31, 32],\n",
      "           [33, 34, 35]],\n",
      "\n",
      "          [[36, 37, 38],\n",
      "           [39, 40, 41],\n",
      "           [42, 43, 44]],\n",
      "\n",
      "          [[45, 46, 47],\n",
      "           [48, 49, 50],\n",
      "           [51, 52, 53]]]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(1, 2, 3, 3, 3) # batch, channel, time, height, width\n",
    "a = torch.tensor( np.arange(0,2*27).reshape(2, 3, 3,3) ).unsqueeze(0)\n",
    "model = Embedding()\n",
    "out = model(a)#.permute(0, 1, 3 ,2).reshape(1, 5, 27)\n",
    "#out = model(a).permute(0, 1, 2, 3 ).reshape(1, 5, 27)\n",
    "print(out.shape, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 27]) tensor([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "          17, 18, 19, 20, 21, 22, 23, 24, 25, 26],\n",
      "         [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n",
      "          44, 45, 46, 47, 48, 49, 50, 51, 52, 53]]], dtype=torch.int32)\n",
      "torch.Size([1, 2, 27]) tensor([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "          17, 18, 19, 20, 21, 22, 23, 24, 25, 26],\n",
      "         [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n",
      "          44, 45, 46, 47, 48, 49, 50, 51, 52, 53]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "out = out.reshape(1, 2, 3* 9)\n",
    "print(out.shape, out)\n",
    "out = out.reshape(1, 2,3,9).permute(0,2,1,3).reshape(1,3,2*9) \n",
    "out = out.reshape(1, 3,2,9).permute(0,2,1,3).reshape(1, 2, 3*9)\n",
    "\n",
    "print(out.shape, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch,  channel, time, height, width = x.size()\n",
    "        basic = torch.ones( (height,1), dtype=torch.int16)\n",
    "        basic2 = torch.ones( (width,1), dtype=torch.int16)\n",
    "        basic2[width//2,0] = 0 \n",
    "        position_embedding = basic@basic2.T + basic2\n",
    "        position_embedding = torch.sqrt(position_embedding)*2.5\n",
    "        x = x + position_embedding\n",
    "        x = x.view(batch, channel, time, height * width) \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, num_heads, num_axis, num_dim):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.num_axis = num_axis\n",
    "        self.num_dim = num_dim\n",
    "\n",
    "        #self.w_q = nn.ModuleList([nn.Linear(self.num_dim, self.num_dim)\n",
    "        #                          for _ in range(self.num_heads)])\n",
    "        #self.w_k = nn.ModuleList([nn.Linear(self.num_dim, self.num_dim) \n",
    "        #                          for _ in range(self.num_heads)])\n",
    "        #self.w_v = nn.ModuleList([nn.Linear(self.num_dim, self.num_dim) \n",
    "        #                          for _ in range(self.num_heads)])\n",
    "        #self.w_o = nn.ModuleList([nn.Linear(self.num_dim, self.num_dim) \n",
    "        #                          for _ in range(self.num_heads)])\n",
    "        self.w_q = nn.Linear(num_axis*self.num_dim, num_axis*self.num_dim)\n",
    "        self.w_k = nn.Linear(num_axis*self.num_dim, num_axis*self.num_dim)\n",
    "        self.w_v = nn.Linear(num_axis*self.num_dim, num_axis*self.num_dim)\n",
    "        self.w_o = nn.Linear(num_axis*self.num_dim, num_axis*self.num_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #q_outputs = []\n",
    "        #k_outputs = []\n",
    "        #v_outputs = []\n",
    "        #for i in range(self.num_heads):\n",
    "        #    # 각 헤드별 선형 투영\n",
    "        #    q_i = self.w_q[i](x[:,i,:,:]) \n",
    "        #    k_i = self.w_k[i](x[:,i,:,:]) \n",
    "        #    v_i = self.w_v[i](x[:,i,:,:]) \n",
    "        #    q_outputs.append(q_i)\n",
    "        #    k_outputs.append(k_i)\n",
    "        #    v_outputs.append(v_i)\n",
    "\n",
    "        #q = torch.stack(q_outputs, dim=1)\n",
    "        #k = torch.stack(k_outputs, dim=1)\n",
    "        #v = torch.stack(v_outputs, dim=1)\n",
    "        x = x.view(-1, self.num_heads, self.num_axis*self.num_dim)\n",
    "        q = self.w_q(x).view(-1, self.num_heads, self.num_axis, self.num_dim)\n",
    "        k = self.w_k(x).view(-1, self.num_heads, self.num_axis, self.num_dim)\n",
    "        v = self.w_v(x).view(-1, self.num_heads, self.num_axis, self.num_dim)\n",
    "\n",
    "        score = torch.matmul(q,k.transpose(-2,-1))\n",
    "        score = score / math.sqrt(self.num_dim)\n",
    "        attention_weights = F.softmax(score, dim=-1)\n",
    "        context = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        #for i in range(self.num_heads):\n",
    "        #    context_i = self.w_o[i](context[:,i,:,:])\n",
    "        #    context[:,i,:,:] = context_i\n",
    "        context = context.view(-1, self.num_heads,self.num_axis*self.num_dim)\n",
    "        context = self.w_o(context).view(-1, self.num_heads, self.num_axis, self.num_dim)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 9]) tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12., 13., 14., 15., 16., 17.],\n",
      "          [18., 19., 20., 21., 22., 23., 24., 25., 26.]],\n",
      "\n",
      "         [[27., 28., 29., 30., 31., 32., 33., 34., 35.],\n",
      "          [36., 37., 38., 39., 40., 41., 42., 43., 44.],\n",
      "          [45., 46., 47., 48., 49., 50., 51., 52., 53.]]]])\n",
      "torch.Size([1, 2, 3, 9]) tensor([[[[ -6.0152,   2.8868,   5.5611,  -1.2698,   5.3441,   7.3263,  -0.8581,\n",
      "             2.0172,   2.5754],\n",
      "          [  5.5988,  -3.8020,   0.3109,  -7.6027,   0.8487,  -3.8262,   3.7686,\n",
      "            -3.9909,  -1.7513],\n",
      "          [ -7.2704,  -6.0137,  -1.9150,   3.8115,   2.4304,   7.8921,   1.5275,\n",
      "             0.9442,  -2.1012]],\n",
      "\n",
      "         [[-13.8452,  11.3892,  16.3185,  -0.7192,  23.6106,  26.4914, -10.4970,\n",
      "             9.6060,   3.5596],\n",
      "          [  6.9093, -11.2839,  -7.1442, -24.7043,  -0.7202, -14.3800,  14.9439,\n",
      "           -10.7903,  -4.1461],\n",
      "          [-29.8874, -26.4242, -10.3746,  15.1422,  10.0584,  20.8193,   1.5905,\n",
      "             3.7553,  -5.2740]]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### attention input test\n",
    "import math\n",
    "a = torch.tensor( np.arange(0,2*27) , dtype=torch.float32).view(1,2, 3, 3,3)\n",
    "out =a.view(1, 2, 3, 9) ##한 시점안의 관련공간에 관한 밴드 attention\n",
    "print(out.shape, out)\n",
    "model = Attention( 2, 3, 9 )\n",
    "out = model(out)\n",
    "print(out.shape, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, expand_ratio=2):\n",
    "        super().__init__()\n",
    "        hidden_dim = dim * expand_ratio\n",
    "        self.w1 = nn.Linear(dim, hidden_dim)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposeTransformerBlock(nn.Module):\n",
    "    def __init__(self,  num_heads, num_axis,num_dim, expand_ratio,tag, dropout=0., attn_drop_rate=0., droppath=0.):\n",
    "        super().__init__()\n",
    "        self.token_norm = nn.LayerNorm(num_dim)\n",
    "        self.token_mixer = Attention(num_heads, num_axis, num_dim)\n",
    "        self.channel_norm = nn.LayerNorm(num_dim)\n",
    "        self.channel_mixer = FeedForward(num_dim, expand_ratio)\n",
    "        self.droppath = DropPath(droppath) if droppath > 0. else nn.Identity()\n",
    "        self.tag = tag\n",
    "    def forward(self, x):\n",
    "        print('transformer',self.tag)\n",
    "        x = self.token_norm(x)\n",
    "        x = self.token_mixer(x)\n",
    "        x = x + self.droppath(x)\n",
    "        x = x + self.droppath(self.channel_mixer(self.channel_norm(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposeTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_blocks=3, \n",
    "                 num_head=10,\n",
    "                 num_axis=12, \n",
    "                 num_dim=9,\n",
    "                 num_classes=6, \n",
    "                 image_size=3,\n",
    "                 patch_size=1, \n",
    "                 expand_ratio=2, \n",
    "                 droppath=0.0, \n",
    "                 dropout=0.0\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.num_axis = num_axis\n",
    "        self.num_dim = num_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.image_size = image_size\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        self.embedding_layer = Embedding()\n",
    "\n",
    "        self.time_layers = nn.Sequential(*[\n",
    "            TransposeTransformerBlock(self.num_head, self.num_axis, self.num_dim,\n",
    "                                       expand_ratio,'time',dropout=dropout, droppath=droppath)  \n",
    "            for _ in range(self.num_blocks)]\n",
    "        )\n",
    "        self.band_layers = nn.Sequential(*[\n",
    "            TransposeTransformerBlock( self.num_axis, self.num_head, self.num_dim, \n",
    "                                      expand_ratio,'band',dropout=dropout, droppath=droppath)  \n",
    "            for _ in range(self.num_blocks)]\n",
    "        )\n",
    "        self.spatial_layers = nn.Sequential(*[\n",
    "            TransposeTransformerBlock( self.num_axis, self.num_dim, self.num_head, \n",
    "                                      expand_ratio,'spatial',dropout=dropout, droppath=droppath)  \n",
    "            for _ in range(self.num_blocks)]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(self.num_head* self.num_axis)\n",
    "        self.head = nn.Linear( self.num_head* self.num_axis , num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "\n",
    "        for idx in range(self.num_blocks):\n",
    "            print('time',x.shape)\n",
    "            x = self.time_layers[idx](x)\n",
    "            x = x.permute(0,2,1,3)\n",
    "            print('band',x.shape)\n",
    "            x = self.band_layers[idx](x)\n",
    "            x = x.permute(0,1,3,2)\n",
    "            print('spatial',x.shape)\n",
    "            x = self.spatial_layers[idx](x)\n",
    "            x = x.permute(0,3,1,2)\n",
    "        x = x.mean([3]).view(-1,self.num_head* self.num_axis )\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time torch.Size([1, 10, 12, 9])\n",
      "transformer time\n",
      "band torch.Size([1, 12, 10, 9])\n",
      "transformer band\n",
      "spatial torch.Size([1, 12, 9, 10])\n",
      "transformer spatial\n",
      "time torch.Size([1, 10, 12, 9])\n",
      "transformer time\n",
      "band torch.Size([1, 12, 10, 9])\n",
      "transformer band\n",
      "spatial torch.Size([1, 12, 9, 10])\n",
      "transformer spatial\n",
      "torch.Size([1, 6]) tensor([[ 1.3707, -0.1266, -0.2153, -0.9284, -0.3243, -0.6217]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = TransposeTransformer(num_blocks=2, \n",
    "                 num_head=10,\n",
    "                 num_axis=12, \n",
    "                 num_dim=9,\n",
    "                 num_classes=6, \n",
    "                 image_size=3,\n",
    "                 patch_size=1, \n",
    "                 expand_ratio=2, \n",
    "                 droppath=0.0, \n",
    "                 dropout=0.0)\n",
    "\n",
    "a = torch.tensor( np.arange(0,12*10*9) , dtype=torch.float32).view(1,10, 12, 3,3)\n",
    "out = model(a)\n",
    "print(out.shape, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 27]) tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "          14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26.],\n",
      "         [27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40.,\n",
      "          41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53.]]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[213], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m out \u001b[38;5;241m=\u001b[39mout\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m27\u001b[39m) \n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape, out)\n\u001b[1;32m----> 5\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_dim)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape, out)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "a = torch.tensor( np.arange(0,2*27) , dtype=torch.float32).view(1,2, 3, 3,3)\n",
    "out = a\n",
    "out =out.view(1, 2, 27) \n",
    "print(out.shape, out)\n",
    "out = out.view(1, 2, self.num_heads, self.split_dim)\n",
    "print(out.shape, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 9]) tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12., 13., 14., 15., 16., 17.],\n",
      "          [18., 19., 20., 21., 22., 23., 24., 25., 26.]],\n",
      "\n",
      "         [[27., 28., 29., 30., 31., 32., 33., 34., 35.],\n",
      "          [36., 37., 38., 39., 40., 41., 42., 43., 44.],\n",
      "          [45., 46., 47., 48., 49., 50., 51., 52., 53.]]]])\n",
      "torch.Size([1, 3, 9]) tensor([[[13.5000, 14.5000, 15.5000, 16.5000, 17.5000, 18.5000, 19.5000,\n",
      "          20.5000, 21.5000],\n",
      "         [22.5000, 23.5000, 24.5000, 25.5000, 26.5000, 27.5000, 28.5000,\n",
      "          29.5000, 30.5000],\n",
      "         [31.5000, 32.5000, 33.5000, 34.5000, 35.5000, 36.5000, 37.5000,\n",
      "          38.5000, 39.5000]]])\n",
      "torch.Size([1, 2, 9]) tensor([[[ 9., 10., 11., 12., 13., 14., 15., 16., 17.],\n",
      "         [36., 37., 38., 39., 40., 41., 42., 43., 44.]]])\n",
      "torch.Size([1, 2, 3]) tensor([[[ 4., 13., 22.],\n",
      "         [31., 40., 49.]]])\n"
     ]
    }
   ],
   "source": [
    "### transformer input shape test\n",
    "a = torch.tensor( np.arange(0,2*27) , dtype=torch.float32).view(1,2, 3, 3,3)\n",
    "model = Embedding()\n",
    "out = model(a)\n",
    "#print(out.shape, out)\n",
    "##시점 간의 관련공간에 관한 밴드 attention => head=image_size\n",
    "##밴드 간의 관련공간에 관한 시점 attention => head=time_size\n",
    "##픽셀 간의 관련 밴드와 시점 attention => head= 줘야 될것 같다?애매.\n",
    "out =out.view(1, 2, 3, 9) \n",
    "print(out.shape, out)\n",
    "print(out.mean([1]).shape,out.mean([1]))\n",
    "print(out.mean([2]).shape,out.mean([2]))\n",
    "print(out.mean([3]).shape,out.mean([3]))\n",
    "out = out.permute(0,2,1,3).contiguous()\n",
    "#print(out.shape, out)\n",
    "out = out.permute(0,1,3,2).contiguous()\n",
    "#print(out.shape, out)\n",
    "out = out.permute(0,3,1,2).contiguous()\n",
    "#print(out.shape, out)\n",
    "out = out.permute(0,2,1,3).contiguous()\n",
    "#print(out.shape, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 9]) tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12., 13., 14., 15., 16., 17.],\n",
      "          [18., 19., 20., 21., 22., 23., 24., 25., 26.]],\n",
      "\n",
      "         [[27., 28., 29., 30., 31., 32., 33., 34., 35.],\n",
      "          [36., 37., 38., 39., 40., 41., 42., 43., 44.],\n",
      "          [45., 46., 47., 48., 49., 50., 51., 52., 53.]]]])\n",
      "torch.Size([1, 2, 27]) tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "          14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26.],\n",
      "         [27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40.,\n",
      "          41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53.]]])\n",
      "torch.Size([1, 3, 18]) tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8., 27., 28., 29., 30., 31.,\n",
      "          32., 33., 34., 35.],\n",
      "         [ 9., 10., 11., 12., 13., 14., 15., 16., 17., 36., 37., 38., 39., 40.,\n",
      "          41., 42., 43., 44.],\n",
      "         [18., 19., 20., 21., 22., 23., 24., 25., 26., 45., 46., 47., 48., 49.,\n",
      "          50., 51., 52., 53.]]])\n",
      "torch.Size([1, 9, 6]) tensor([[[ 0.,  9., 18., 27., 36., 45.],\n",
      "         [ 1., 10., 19., 28., 37., 46.],\n",
      "         [ 2., 11., 20., 29., 38., 47.],\n",
      "         [ 3., 12., 21., 30., 39., 48.],\n",
      "         [ 4., 13., 22., 31., 40., 49.],\n",
      "         [ 5., 14., 23., 32., 41., 50.],\n",
      "         [ 6., 15., 24., 33., 42., 51.],\n",
      "         [ 7., 16., 25., 34., 43., 52.],\n",
      "         [ 8., 17., 26., 35., 44., 53.]]])\n"
     ]
    }
   ],
   "source": [
    "### transformer input shape test\n",
    "a = torch.tensor( np.arange(0,2*27) , dtype=torch.float32).view(1,2, 3, 3,3)\n",
    "model = Embedding()\n",
    "out = model(a)\n",
    "print(a.shape, a)\n",
    "##시점 간의 관련공간에 관한 밴드 attention => head=image_size\n",
    "out =out.view(1, 2, 27) \n",
    "print(out.shape, out)\n",
    "##밴드 간의 관련공간에 관한 시점 attention => head=time_size\n",
    "out = out.view(1, 2, 3, 9).permute(0,2,1,3).contiguous().view(1,3,2*9) \n",
    "print(out.shape, out)\n",
    "##픽셀 간의 관련 밴드와 시점 attention => head= 줘야 될것 같다?애매.\n",
    "out = out.view(1, 3, 2,9 ).permute(0,3,2,1).contiguous().view(1,9,2*3) \n",
    "print(out.shape, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
